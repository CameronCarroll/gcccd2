% Chem 141 Lab 1
% Cameron Carroll


\documentclass{article}

\usepackage[version=3]{mhchem} % Package for chemical equation typesetting
\usepackage{tabularx}
\usepackage{listings}
\lstset{language=Matlab}

\usepackage{graphicx} % Required for the inclusion of images

\setlength\parindent{0pt} % Removes all indentation from paragraphs

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

%\usepackage{times} % Uncomment to use the Times New Roman font

%----------------------------------------------------------------------------------------
% DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Density: A Study in Precision and Accuracy \\ Grossmont College \\ Chemistry 141 (2526)} % Title



\author{Cameron \textsc{Carroll}}

\date{\today}

\begin{document}

\maketitle

\begin{center}
\begin{tabular}{l r}
Date Performed: & August 19 \& 21 \\
Partner: & Joe Ortiz \\
Professor: & John Oakes
\end{tabular}
\end{center}

\newpage


%----------------------------------------------------------------------------------------
% SECTION 1
%----------------------------------------------------------------------------------------

\section{Introduction}
\subsection{Objective}
  \paragraph{}
    This experiment aims to teach some basic statistical analysis for lab
    data.   It covers precision, accuracy, rejection, validity, and
    least-squares analysis.
\subsection{Theory}
  \paragraph{Precision \& Accuracy}
    Precision, calculated as the standard deviation in a dataset,
    represents an average of how far each datum is from its peers.
    This shows the random error in the measurements, most likely an
    artifact of the level of precision of the instruments used. \\
    \begin{center}$\text{standard deviation} = \sigma = 
    \sqrt{\frac{\Sigma (x-\bar{x})^2}{n-1}}$\end{center}
    \mbox{}\\
    Accuracy, calculated as percent error from a literature value,
    is an indicator of systematic error. This indicates human error
    in conducting the experiment which won't contribute to the
    standard deviation. \\
    \begin{center}$\text{percent error} = (100\%)
    \frac{\text{measured\ value} - \text{theoretical\ value}}{theoretical\ value}$\end{center}
   \paragraph{Validity \& Rejection}
    We analyze one additional statistical property, validity
    (via the `t test' and a rejection test (`Q test'.)
    \\
    Validity is a measure of to what extent we believe cause
    and effect are related. Via the `t test,' we can declare
    a correlation valid or invalid to a certain degree of
    confidence: \\
    \begin{center}$t=\frac{\bar{x_1}-\bar{x_2}}{s_p}%mult
    \sqrt{\frac{N_1N_2}{N_1+N_2}}$\end{center}
    \begin{center}$S_p = \sqrt{\frac{\Sigma (x_{i1}-\bar{x_1})^2 +
    \Sigma(x_{i2}-\bar{x_2})^2}{N_1 + N_2 - 2}}$\end{center}
    \mbox{}\\
    The rejection test in question, the `Q test,' is a quick
    and convenient method of checking the validity of an outlying
    datum. For outliers data[0] and data[-1] in a sorted dataset, \\
    \begin{center}$Q=\frac{gap}{range}$\end{center}
    \begin{center}$Q[0]=\frac{data[1]-data[0]}{data[-1]-data[0]}$\end{center}
    \mbox{}\\
    Or we can say that the gap is the difference between our outlier
    and the nearest value divided into the difference between outlier
    and furthest value.
   \paragraph{Least-Squares Analysis}
    Least-squares is a method to determine the best straight-line fit through a set of data. The idea is to minimize the sum of squared residuals, residuals being differences between an observed value and the model value. In order to determine the line, we calculate a slope and intercept:
    \begin{center}$m = \frac{\Sigma (x_i - \bar{x})(y_i - \bar{y})}{\Sigma (x_i - \bar{x})^2}$\end{center}
    \begin{center}$b = \bar{y} - m \bar{x}$\end{center}
    The uncertainties in slope and intercept can be calculated after the standard deviation for the dependent variable:
    \begin{center}$s_y = \sqrt{\frac{[\Sigma y_1^2 - \frac{[(\Sigma y_i)^2}{N}] - m^2 [\Sigma x_i^2 - \frac{(\Sigma x_i)^2}{N}]}{N-2}}$\end{center}
    \begin{center}$s_m = \sqrt{\frac{s_y^2}{\Sigma x_i^2 - \frac{(\Sigma x_i)^2}{N}}}$\end{center}
    \begin{center}$s_b = s_y \sqrt{\frac{1}{N-\frac{(\Sigma x_i)^2}{\Sigma x_i^2}}}$\end{center}

%----------------------------------------------------------------------------------------
% SECTION 2
%----------------------------------------------------------------------------------------
\newpage
\section{Results and Calculations}
\subsection{Bead Density Analysis} \mbox{}\\
    \begin{tabularx}{450pt}{|X | X |}
        \hline
        Bead Mass: & $6.32 \pm 0.03\ g$\\
        Bead Volume: & $2.53 \pm 0.08\ mL$\\
        Bead Density: & $2.50 \pm 0.08\ \frac{g}{mL}$ \\
        Density \% Error: & $3.23 \%$ \\
        \hline
    \end{tabularx}
\subsection{Bean Sprouts \& UV Light} \mbox{}\\
    \begin{tabularx}{450pt}{|X | X |}
        \hline
        Average \# Sprouted (Without UV): & 79.286\\
        Average \# Sprouted (With UV): & 70.857\\
        `t test' value: & 2.6151\\
        $t_{crit (90\%)}$ & $1.943$ \\
        \hline
    \end{tabularx}
\subsection{Least Squares Analysis} \mbox{}\\
    \begin{tabularx}{450pt}{|X | X |}
        \hline
        Average Mass & 111.39 g \\
        Average Volume & 23.342 mL \\
        m (slope) & $0.99 \pm 0.09\ \frac{g}{mL}$ \\
        b (intercept) & $88.248 \pm 10.32$ g \\
        \hline
    \end{tabularx}

%----------------------------------------------------------------------------------------
% SECTION 3
%----------------------------------------------------------------------------------------
\newpage

\section{Discussion}
    \subsection{Bead Density Analysis}
        \paragraph{Sources of Error: } The most likely source of error here would be miscounting the glass beads and misreading the buret. Contributing factors include precision of balance, aforementioned precision of buret, and any bubbles preventing the beads from completely settling. All of these are random errors except for the bubbles, as they could influence the data either way.
        \paragraph{Results: } The experimental value for density, 2.498 g/mL was 3.23\% higher than the theoretical value of 2.42 g/mL. This seems like a reasonable deviation given the possible factors involved. 
    \subsection{Bean Sprouts \& UV Light}
        \paragraph{Sources of Error: }Assumed to be none, since this was a dry experiment.
        \paragraph{Results: } The question of whether UV exposure was correlated with lower sprout rates is answered with a `t test.' This test yields a value of 2.6151, far above our 90\% certainty threshold of 1.943. Therefore we can say that the cause and effect are, indeed, correlated.
    \subsection{Least Squares Analysis}
        \paragraph{Sources of Error: } In this section, uncertainty stems from the two measurements: volume (graduated cylinder) and mass (electronic scale.) These are combined into an uncertainty for density.
        \paragraph{Results: } The values for slope and intercept obtained by hand are exactly equal to those obtained by the Octave polyfit() function, using a polynomial of degree 1. In addition, the value for slope is only 0.58\% off of the literature value for the density of water at that temperature. The uncertainty in the intercept, representing the initial mass with no volume of water, is very large however. (Nearly 12\% of  88.248 g.)


%----------------------------------------------------------------------------------------
% SECTION 4
%----------------------------------------------------------------------------------------
\newpage
\section{Conclusion}
    \subsection{Bead Density Analysis}
        \paragraph{} c1
    \subsection{Bean Sprouts \& UV Light}
        \paragraph{} c2
    \subsection{Least Squares Analysis}
        \paragraph{} c3

%----------------------------------------------------------------------------------------
% SECTION 5
%----------------------------------------------------------------------------------------
\newpage
\section{Post-Lab Questions}
    \subsection{Part A}
        \subsubsection{}
            \paragraph{} Yes, the value of random error is enough to explain the difference. Density was found to be $2.50 \pm 0.08$ g/mL, and the literature value was said to be 2.42 g/mL.
        \subsubsection{}
            \paragraph{} No, the literature value is within reach of the experimental value $\pm$ the uncertainty. Therefore there is no need to invoke systematic error.
        \subsubsection{}
            \paragraph{} Random error could have come from miscounting the beads or scale fluctuations based on air currents.
        \subsubsection{}
            \paragraph{} Systematic error could have come either from misreading the volume (wrong point on meniscus) or forgetting to tare the scale. If someone read volumes from the top of the meniscus instead of the bottom, they would consistently get higher volumes than expected and yield a lower density compared to the correct value. Likewise forgetting to tare the scale would result in higher masses and a higher density than would be correct.

%----------------------------------------------------------------------------------------
% SECTION 6
%----------------------------------------------------------------------------------------
\newpage
\section{Code Reference}
    \subsection{(A)}
        \begin{lstlisting}
data = [8.6152 8.4690 8.2965 8.4817 8.5172 8.4314 8.5335 8.4779 8.5124 8.4688];
sorted = sort(data);
dataset = sorted(2:9) # Remove outliers (8.6152 and 8.2965);
tare_weight = 2.1692;
dataset = dataset - tare_weight;

# Part 1, Calculation 1 (Average):
average = sum(dataset) / length(dataset)
average_mass = average

# Part 2, Calculation 2 (Std Dev):
diffs = dataset - average
diffs_squared = diffs.^2
degrees_of_freedom = length(diffs_squared) - 1
mass_stdev = sqrt(sum(diffs_squared) / degrees_of_freedom)
canned_stdev = std(dataset)
        \end{lstlisting}

    \subsection{(B)}
        \begin{lstlisting}
values = [22.14 20.13 16.99 14.43 11.95 9.38 7.01 4.43 1.88];
second_run_values = [28.39 25.78 23.27];
data = [values(1)-values(2) values(2)-values(3) values(3)-values(4)...
 values(4)-values(5) values(5)-values(6) values(6)-values(7)...
  values(7)-values(8) values(8)-values(9)...
   second_run_values(1)-second_run_values(2)...
    second_run_values(2)-second_run_values(3)];

sorted = sort(data);

# Calculate Q values for outliers (first and last values in sorted array)
q1 = (sorted(2)-sorted(1)) / (sorted(10)-sorted(1))
q10 = (sorted(10)-sorted(9)) / (sorted(10) - sorted(1))

dataset = sorted(2:9)

average = mean(dataset)
average_volume = average
diffs = dataset - average
diffs_squared = diffs.^2
degrees_of_freedom = length(diffs_squared) - 1
volume_stdev = sqrt(sum(diffs_squared) / degrees_of_freedom)
canned_stdev = std(dataset)
        \end{lstlisting}

    \subsection{(C)}
        \begin{lstlisting}
density = average_mass / average_volume
theoretical_density = 2.42
percent_error = 100 * (density - theoretical_density) / theoretical_density
random_error = density * sqrt((mass_stdev / average_mass)^2 +...
 (volume_stdev / average_volume)^2)
        \end{lstlisting}

    \subsection{(D)}
        \begin{lstlisting}
without = [87 72 88 81 69 78 80];
with = [71 64 80 69 70 70 72];

avg_without = mean(without)
avg_with = mean(with)
        \end{lstlisting}

    \subsection{(E)}
        \begin{lstlisting}
Sp = sqrt((sum((without - avg_without).^2) + sum((with - avg_with).^2)) /...
 (length(without) + length(with) - 2))

t = (avg_without - avg_with) / Sp * sqrt((length(without)*length(with)) /...
 (length(without) + length(with)))
        \end{lstlisting}

    \subsection{(F)}
        \begin{lstlisting}
volumes = [0 8.0 17.9 28.75 37.70 47.70];
masses = [88.543 96.015 105.895 116.529 125.625 135.757];
tare = 88.543;

average_volume = mean(volumes)
average_mass = mean(masses)
m = sum((volumes - average_volume) .* (masses - average_mass)) /...
 sum((volumes - average_volume).^2)
deps = volumes;
inds = masses;
N = length(deps)
s_y = sqrt(((sum(deps.^2) - sum(deps)^2 / N) - m^2 * (sum(inds.^2) -...
 sum(inds)^2 / N))/(N-2))
s_m = sqrt(s_y^2 / (sum(inds.^2) - sum(inds)^2 / N))
s_b = s_y * sqrt(1/(N-(sum(inds)^2/sum(inds.^2))))

b = average_mass - m * average_volume
rho = 0.997394

percent_error = (m - rho) / rho * 100
canned_answer = polyfit(volumes, masses, 1)
        \end{lstlisting}

\end{document}